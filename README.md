# hubness-reduction-improves-sbert-semantic-spaces

Arxiv link: COMING

## Description
TODO: Let people know what your project can do specifically. Provide context and add a link to any reference visitors might be unfamiliar with. A list of Features or a Background subsection can also be added here. If there are alternatives to your project, this is a good place to list differentiating factors.

## Installation guide
The outline of the installation is the following:

**1. Create and activate conda environment**

**2. Install scikit hubness package**

**1. Create and activate conda environment**

If you are installing on a linux machine with GPU, use the linux_gpu.yml file provided and the commands:
```
conda create -f linux_gpu.yml
conda activate s_bert_hub
```

**2. Install scikit hubness package** 

Run the command:
```
python -m pip install git+https://github.com/VarIr/scikit-hubness.git
```

## Datasets

Due to the size, the Yahoo Answers dataset is not included in this repo. It can be downloaded from https://www.kaggle.com/datasets/yacharki/yahoo-answers-10-categories-for-nlp-csv. 


## Usage
TODO: Use examples liberally, and show the expected output if you can. It's helpful to have inline the smallest example of usage that you can demonstrate, while providing links to more sophisticated examples if they are too long to reasonably include in the README.

## Authors and acknowledgment
TODO: Show your appreciation to those who have contributed to the project.

## License
Apache License 2.0. See LICENSE.

